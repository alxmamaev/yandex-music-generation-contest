{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(\"abc.yttm\")\n",
    "test_paths = get_training_files('testset/abc')\n",
    "test_texts = [read_abc(p) for p in test_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(tokenizer.vocab_size())\n",
    "checkpoint = torch.load(\"pytorch_model.bin\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1 T: M:4/4 L:1/8 Q:1/4=120 K:F V:1 @ [F3/2C3/2F,3/2-] F,/2- [AF-CA,-F,-]  [F/2A,/2-F,/2-]  [A,/2F,/2-] F,- [A/2F/2C/2-A,/2-F,/2-]  [C/2A,/2F,/2-]  [D/2F,/2-]  [G,/2-F,/2D,/2-]  [G,-D,-]  |  [G/2=B,/2G,/2-D,/2-]  [D/2G,/2-D,/2-]  [G,/2D,/2-]  [G/2D/2-=B,/2D,/2-]  [D-D,-]  [D/2=B,/2-D,/2-]  [=B,-D,-]  [=B,3/2G,3/2-D,3/2-]  [G3/2D3/2=B,3/2G,3/2-D,3/2-]  [G,/2-D,/2-]  |  [G,-D,-]  [D/2G,/2-D,/2] G,3/2- [G,3D,3-]  [G/2D/2-=B,/2D,/2-]  [D-D,-]  [G/2D/2-=B,/2-D,/2-]  |  [D/2-=B,/2D,/2-]  [D/2D,/2]  [A,3/2-E,3/2-]  [E/2_D/2A,/2-E,/2-]  [A,/2E,/2-] E,/2- [E3/2-A,3/2-E,3/2-]  [G/2-E/2-_D/2-A,/2E,/2]  [GE_D] =D,- | D,/2- [A,/2F,/2-D,/2-]  [F,D,-]  [F/2D/2-D,/2-]  [D-D,-]  [F/2D/2A,/2-D,/2-]  [A,/2-D,/2] A,/2D,3/2- [F3/2D3/2A,3/2D,3/2-]  | D,3/2- [F/2A,/2D,/2-] D,/2x/2D,3/2- [F,/2-D,/2-]  [A,/2F,/2D,/2-] D,/2- [F/2D/2-A,/2D,/2-]  [DD,-]  [F/2A,/2-D,/2-]  |  [A,2D,2-] D,/2- [F3/2D3/2-A,3/2D,3/2-]  [D3/2D,3/2]  [F/2F,/2-]  [D/2C/2A,/2F,/2] x/2D,- | D,/2x/2 [A,/2F,/2-]  [F,D,]  [F/2D/2-A,/2] D- [F/2D/2A,/2-] A,- [A,3/2-D,3/2-]  [F/2-D/2-C/2-A,/2-D,/2]  [F/2-D/2-C/2A,/2]\n"
     ]
    }
   ],
   "source": [
    "test_text = test_texts[893]\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 14872, 770, 10147, 1387, 11048], [2, 1798, 3617, 5040, 5044, 1865], [2, 69]]\n",
      "[2, 726, 258]\n",
      "\n",
      "[[2, 1798, 3617, 5040, 5044, 1865], [2, 24217, 1865, 4216, 1682, 3617], [2, 69]]\n",
      "[2, 3470, 4178]\n",
      "\n",
      "[[2, 24217, 1865, 4216, 1682, 3617], [2, 3470, 4178, 12384, 770, 3043], [2, 69]]\n",
      "[2, 1146, 5145]\n",
      "\n",
      "[[2, 3470, 4178, 12384, 770, 3043], [2, 1146, 5145, 770, 3043, 3470], [2, 69]]\n",
      "[2, 770, 2122]\n",
      "\n",
      "[[2, 1146, 5145, 770, 3043, 3470], [2, 770, 2122, 1146, 3043, 366], [2, 69]]\n",
      "[2, 1146, 2122]\n",
      "\n",
      "[[2, 770, 2122, 1146, 3043, 366], [2, 1146, 2122, 770, 3043, 366], [2, 69]]\n",
      "[2, 1146, 3043]\n",
      "\n",
      "[[2, 1146, 2122, 770, 3043, 366], [2, 1146, 3043, 366, 278, 86], [2, 69]]\n",
      "[2, 770, 2122]\n",
      "\n",
      "[[2, 1146, 3043, 366, 278, 86], [2, 770, 2122, 1146, 3043, 366], [2, 69]]\n",
      "[2, 1146, 2122]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys, notes = test_text.split(\" @ \")\n",
    "notes = notes.split(\" | \")\n",
    "\n",
    "gen_tokens = None\n",
    "for i in (range(8)):\n",
    "    context = keys + \"@\" + \" | \".join(notes[-8:])\n",
    "    context_tokens = tokenizer.encode(context, bos=True, eos=True)\n",
    "    input_tokens = torch.tensor(context_tokens, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    bad_tokens = [tokenizer.encode(i, bos=True, eos=True)[:3] for i in notes[-2:]] + [[2, 69]]\n",
    "    \n",
    "    gen_tokens = model.generate(input_ids=input_tokens.unsqueeze(0), \n",
    "                                max_length=35, \n",
    "                                num_beams=8, \n",
    "                                no_repeat_ngram_size=1, \n",
    "                                early_stopping=True,\n",
    "                                temperature=0.3,\n",
    "                                bos_token_id=2, \n",
    "                                eos_token_id=3, \n",
    "                                pad_token_id=0,\n",
    "                                bad_words_ids=bad_tokens)[0].tolist()\n",
    "    \n",
    "    print(bad_tokens)\n",
    "    print(gen_tokens[:3])\n",
    "    print()\n",
    "    notes += tokenizer.decode(gen_tokens, ignore_ids=[0,1,2,3])[0].split(\" | \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[F3/2C3/2F,3/2-]F,/2-[AF-CA,-F,-][F/2A,/2-F,/2-][A,/2F,/2-]F,-[A/2F/2C/2-A,/2-F,/2-][C/2A,/2F,/2-][D/2F,/2-][G,/2-F,/2D,/2-][G,-D,-]|\n",
      "[G/2=B,/2G,/2-D,/2-][D/2G,/2-D,/2-][G,/2D,/2-][G/2D/2-=B,/2D,/2-][D-D,-][D/2=B,/2-D,/2-][=B,-D,-][=B,3/2G,3/2-D,3/2-][G3/2D3/2=B,3/2G,3/2-D,3/2-][G,/2-D,/2-]|\n",
      "[G,-D,-][D/2G,/2-D,/2]G,3/2-[G,3D,3-][G/2D/2-=B,/2D,/2-][D-D,-][G/2D/2-=B,/2-D,/2-]|\n",
      "[D/2-=B,/2D,/2-][D/2D,/2][A,3/2-E,3/2-][E/2_D/2A,/2-E,/2-][A,/2E,/2-]E,/2-[E3/2-A,3/2-E,3/2-][G/2-E/2-_D/2-A,/2E,/2][GE_D]=D,-|\n",
      "D,/2-[A,/2F,/2-D,/2-][F,D,-][F/2D/2-D,/2-][D-D,-][F/2D/2A,/2-D,/2-][A,/2-D,/2]A,/2D,3/2-[F3/2D3/2A,3/2D,3/2-]|\n",
      "D,3/2-[F/2A,/2D,/2-]D,/2x/2D,3/2-[F,/2-D,/2-][A,/2F,/2D,/2-]D,/2-[F/2D/2-A,/2D,/2-][DD,-][F/2A,/2-D,/2-]|\n",
      "[A,2D,2-]D,/2-[F3/2D3/2-A,3/2D,3/2-][D3/2D,3/2][F/2F,/2-][D/2C/2A,/2F,/2]x/2D,-|\n",
      "D,/2x/2[A,/2F,/2-][F,D,][F/2D/2-A,/2]D-[F/2D/2A,/2-]A,-[A,3/2-D,3/2-][F/2-D/2-C/2-A,/2-D,/2][F/2-D/2-C/2A,/2]|\n",
      "[F/2D/2-C/2A,/2]D-[F/2D/2A,/2-]A,-[A,/2F,/2-]F,3/2-[F3/2C3/2A,3/2F,3/2-]F,/2-[F/2C/2A,/2F,/2]x/2|\n",
      "D,3/2-[F,/2-D,/2-][A,/2F,/2D,/2-]D,/2-[F/2D/2A,/2D,/2-]D,-[A,/2D,/2-]D,/2[F/2D/2A,/2]x/2D,-|\n",
      "D,-[F,/2D,/2-]D,/2-[F/2D/2A,/2D,/2-]D,3/2-[A,/2D,/2-]D,[F/2D/2A,/2]x/2D,-|\n",
      "D,/2-[A,/2D,/2-]D,-[F/2D/2A,/2D,/2-]D,[F/2D/2A,/2]x/2D,3/2-[D/2A,/2D,/2-]D,3/2-|\n",
      "D,-[A,/2D,/2-]D,/2-[F/2D/2A,/2D,/2-]D,3/2A,2-A,/2D,-|\n",
      "D,-[F/2D/2A,/2D,/2-]D,3/2A,3/2-[F/2D/2A,/2]x/2D,-[A,/2D,/2-]D,/2-|\n",
      "D,/2-[A,/2D,/2-]D,-[F/2D/2A,/2D,/2-]D,3/2A,2-A,/2D,-|\n",
      "D,-[A,/2D,/2-]D,/2-[F/2D/2A,/2D,/2-]D,3/2A,2-A,/2D,-\n"
     ]
    }
   ],
   "source": [
    "print(\"|\\n\".join(notes).replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X:1 T: M:4/4 L:1/8 Q:1/4=120 K:F V:1'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: ['<BOS> [^F/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-D/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2D/2=A,/2] [ED^G,] A,/2- [D/2A,/2] [E/2A,/2]<EOS><PAD><PAD>']\n",
      "1: ['<BOS> [^F/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-D/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2D/2=A,/2] [ED^G,] A,/2- [E/2A,/2] [=F/2A,/2] x/2<EOS>']\n",
      "2: ['<BOS> [^F/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-D/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2D/2=A,/2] [ED^G,] A,/2- [E/2A,/2] [=F/2A,/2-] [C/2A,/2]<EOS>']\n",
      "3: ['<BOS> [^F/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-D/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2D/2=A,/2] [ED^G,] A,/2- [E/2A,/2] [=F/2A,/2-] [D/2A,/2]<EOS>']\n",
      "4: ['<BOS> [^F/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-D/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2D/2=A,/2] [ED^G,] A,/2- [E/2A,/2] [=F/2A,/2] C/2<EOS>']\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=40, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=1, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True,\n",
    "    temperature=0.7,\n",
    "    bos_token_id=2, eos_token_id=3, pad_token_id=0\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> X:1 T: M:4/4 L:1/8 Q:1/4=50 K:D%2sharps V:1 @ [E/2D/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-E/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2-D/2-^A,/2] [=F/2D/2=A,/2] [EDG,] [ED^G,] [E/2-A,/2-] | [E/2A,/2] A,,A,D [=F/2-D,,/2] [=F/2-A,/2=F,/2] [=F/2A,/2=F,/2] [E/2D/2A,/2=F,/2] [C/2-D,,/2] [D/2C/2^A,/2=G,/2] [E/2-D,,/2] [E/2C/2^A,/2G,/2] [D/2-D,,/2] | [D/2=A,/2=F,/2] [A,/2=F,/2] [A,/2=F,/2] [D/2C/2A,/2=F,/2] [E/2D/2] [^F/2=F/2] [^G/2=G/2] [A/2-D,,/2-] [A/2-A,/2=F,/2D,,/2-] [A/2D/2D,,/2] [A/2A,/2=F,/2] [^A/2-D,,/2-] [^A/2^A,/2G,/2D,,/2-] [d/2-D,,/2] [d/2^A,/2G,/2] [=A/2-=F,,/2] | [A/2A,/2=F,/2] xA,D [=F/2-D,,/2] [=F/2A,/2=F,/2] [G/2=F/2A,/2=F,/2] [E/2D/2A,/2=F,/2] [C/2-A,,/2] [C/2G,/2E,/2] [C/2-A,,/2] [C/2G,/2E,/2] [=C/2-A,/2-^D,/2-^F,,/2-] | [=CA,^D,F,,] x/2 [^A,E,G,,] [=A,E,A,,] [^G,-=D,^A,,-] [^G,/2E,/2-^A,,/2] E,/2 [=A,=F,A,,] [^C=G,E,A,,] [D/2-=F,/2-D,/2-D,,/2-] | [D/2=F,/2D,/2D,,/2] x [A/2-=F/2-=F,/2] [A/2=F/2G,/2] [A/2-=F/2-A,/2] [A/2=F/2^A,/2] [G/2-=F/2-=C/2] [G/2=F/2=C,/2] [G/2-E/2-] [G/2E/2^C,/2] [=F/2-E/2-D,/2] [=F/2E/2E,/2] [=F/2-D/2-=F,/2] [=F/2D/2G,/2] [E/2-D/2-=A,/2] | [E/2D/2A,,/2] [A/2-C/2-] [A/2C/2A,/2] A/2- [A/2D/2^A,/2] [G/2-E/2] [G/2D/2] [G/2-C/2-] [G/2C/2=A,/2] [=F/2-D/2-^A,/2] [=F/2D/2=A,/2] [EDG,] [ED^G,] [E/2-A,/2-] | [E/2A,/2] A,,A,D [=F/2-D,,/2] [=F/2-A,/2=F,/2] [=F/2A,/2=F,/2] [E/2D/2A,/2=F,/2] [C/2-D,,/2] [D/2C/2^A,/2=G,/2] [E/2-D,,/2] [E/2C/2^A,/2G,/2] [D/2-D,,/2]<EOS>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
